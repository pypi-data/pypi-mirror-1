====================
Benchmarks and Speed
====================

:Author:
  Stefan Behnel

.. meta::
  :description: Performance evaluation of lxml and ElementTree 
  :keywords: lxml performance, lxml.etree, lxml.objectify, benchmarks, ElementTree


As an XML library, lxml.etree is very fast.  It is also slow.  As with
all software, it depends on what you do with it.  Rest assured that
lxml is fast enough for most applications, so lxml is probably
somewhere between 'fast enough' and 'the best choice' for yours.  Read
some messages_ from happy_ users_ to see what we mean.

.. _messages: http://permalink.gmane.org/gmane.comp.python.lxml.devel/3250
.. _happy: http://article.gmane.org/gmane.comp.python.lxml.devel/3246
.. _users: http://thread.gmane.org/gmane.comp.python.lxml.devel/3244/focus=3244

This text describes where lxml.etree (abbreviated to 'lxe') excels, gives
hints on some performance traps and compares the overall performance to the
original ElementTree_ (ET) and cElementTree_ (cET) libraries by Fredrik Lundh.
The cElementTree library is a fast C-implementation of the original
ElementTree.

.. _ElementTree:  http://effbot.org/zone/element-index.htm
.. _cElementTree: http://effbot.org/zone/celementtree.htm

.. contents::
.. 
   1  How to read the timings
   2  Bad things first
   3  Parsing and Serialising
   4  The ElementTree API
   5  Tree traversal
   6  XPath
   7  lxml.objectify


General notes
=============

First thing to say: there *is* an overhead involved in having a DOM-like C
library mimic the ElementTree API.  As opposed to ElementTree, lxml has to
generate Python representations of tree nodes on the fly when asked for them,
and the internal tree structure of libxml2 results in a higher maintenance
overhead than the simpler top-down structure of ElementTree.  What this means
is: the more of your code runs in Python, the less you can benefit from the
speed of lxml and libxml2.  Note, however, that this is true for most
performance critical Python applications.  No one would implement fourier
transformations in pure Python when you can use NumPy.

The up side then is that lxml provides powerful tools like tree iterators,
XPath and XSLT, that can handle complex operations at the speed of C.  Their
pythonic API in lxml makes them so flexible that most applications can easily
benefit from them.


How to read the timings
=======================

The statements made here are backed by the (micro-)benchmark scripts
`bench_etree.py`_, `bench_xpath.py`_ and `bench_objectify.py`_ that come with
the lxml source distribution.  They are distributed under the same BSD license
as lxml itself, and the lxml project would like to promote them as a general
benchmarking suite for all ElementTree implementations.  New benchmarks are
very easy to add as tiny test methods, so if you write a performance test for
a specific part of the API yourself, please consider sending it to the lxml
mailing list.

The timings cited below compare lxml 2.0 final (with libxml2 2.6.31)
to the January 2008 SVN trunk versions of ElementTree (1.3alpha) and
cElementTree (1.2.7).  They were run single-threaded on a 1.8GHz Intel
Core Duo machine under Ubuntu Linux 7.10 (Gutsy).  The C libraries
were compiled with the same platform specific optimisation flags.  The
Python interpreter (2.5.1) was used as provided by the distribution.

.. _`bench_etree.py`:     http://codespeak.net/svn/lxml/branch/lxml-1.3/benchmark/bench_etree.py
.. _`bench_xpath.py`:     http://codespeak.net/svn/lxml/branch/lxml-1.3/benchmark/bench_xpath.py
.. _`bench_objectify.py`: http://codespeak.net/svn/lxml/branch/lxml-1.3/benchmark/bench_objectify.py

The scripts run a number of simple tests on the different libraries, using
different XML tree configurations: different tree sizes (T1-4), with or
without attributes (-/A), with or without ASCII string or unicode text
(-/S/U), and either against a tree or its serialised XML form (T/X).  In the
result extracts cited below, T1 refers to a 3-level tree with many children at
the third level, T2 is swapped around to have many children below the root
element, T3 is a deep tree with few children at each level and T4 is a small
tree, slightly broader than deep.  If repetition is involved, this usually
means running the benchmark in a loop over all children of the tree root,
otherwise, the operation is run on the root node (C/R).

As an example, the character code ``(SATR T1)`` states that the benchmark was
running for tree T1, with plain string text (S) and attributes (A).  It was
run against the root element (R) in the tree structure of the data (T).

Note that very small operations are repeated in integer loops to make them
measurable.  It is therefore not always possible to compare the absolute
timings of, say, a single access benchmark (which usually loops) and a 'get
all in one step' benchmark, which already takes enough time to be measurable
and is therefore measured as is.  An example is the index access to a single
child, which cannot be compared to the timings for ``getchildren()``.  Take a
look at the concrete benchmarks in the scripts to understand how the numbers
compare.


Parsing and Serialising
=======================

Serialisation is an area where lxml excels.  The reason is that it
executes entirely at the C level, without any interaction with Python
code.  The results are rather impressive, especially for UTF-8, which
is native to libxml2.  While 20 to 40 times faster than (c)ElementTree
1.2, lxml is still more than 5 times as fast as the much improved
ElementTree 1.3::

  lxe: tostring_utf16  (SATR T1)   19.0921 msec/pass
  cET: tostring_utf16  (SATR T1)  129.8430 msec/pass
  ET : tostring_utf16  (SATR T1)  136.1301 msec/pass

  lxe: tostring_utf16  (UATR T1)   20.4630 msec/pass
  cET: tostring_utf16  (UATR T1)  130.1570 msec/pass
  ET : tostring_utf16  (UATR T1)  136.3101 msec/pass

  lxe: tostring_utf16  (S-TR T2)   18.8632 msec/pass
  cET: tostring_utf16  (S-TR T2)  136.9388 msec/pass
  ET : tostring_utf16  (S-TR T2)  143.9550 msec/pass

  lxe: tostring_utf8   (S-TR T2)   14.4310 msec/pass
  cET: tostring_utf8   (S-TR T2)  137.0859 msec/pass
  ET : tostring_utf8   (S-TR T2)  144.3110 msec/pass

  lxe: tostring_utf8   (U-TR T3)    2.6381 msec/pass
  cET: tostring_utf8   (U-TR T3)   52.1040 msec/pass
  ET : tostring_utf8   (U-TR T3)   53.1070 msec/pass

For parsing, on the other hand, the advantage is clearly with
cElementTree.  The (c)ET libraries use a very thin layer on top of the
expat parser, which is known to be extremely fast::

  lxe: parse_stringIO  (SAXR T1)  144.1851 msec/pass
  cET: parse_stringIO  (SAXR T1)   14.4269 msec/pass
  ET : parse_stringIO  (SAXR T1)  245.9190 msec/pass

  lxe: parse_stringIO  (S-XR T3)    5.6100 msec/pass
  cET: parse_stringIO  (S-XR T3)    5.3229 msec/pass
  ET : parse_stringIO  (S-XR T3)   82.4831 msec/pass

  lxe: parse_stringIO  (UAXR T3)   23.4420 msec/pass
  cET: parse_stringIO  (UAXR T3)   30.2689 msec/pass
  ET : parse_stringIO  (UAXR T3)  165.7169 msec/pass

While about as fast for smaller documents, the expat parser allows cET
to be up to 10 times faster than lxml on plain parser performance for
large input documents.  Similar timings can be observed for the
``iterparse()`` function::

  lxe: iterparse_stringIO  (SAXR T1)  160.3689 msec/pass
  cET: iterparse_stringIO  (SAXR T1)   19.1891 msec/pass
  ET : iterparse_stringIO  (SAXR T1)  274.8971 msec/pass

  lxe: iterparse_stringIO  (UAXR T3)   24.9629 msec/pass
  cET: iterparse_stringIO  (UAXR T3)   31.7740 msec/pass
  ET : iterparse_stringIO  (UAXR T3)  173.8000 msec/pass

However, if you benchmark the complete round-trip of a serialise-parse
cycle, the numbers will look similar to these::

  lxe: write_utf8_parse_stringIO  (S-TR T1)  160.0718 msec/pass
  cET: write_utf8_parse_stringIO  (S-TR T1)  207.6778 msec/pass
  ET : write_utf8_parse_stringIO  (S-TR T1)  450.2120 msec/pass

  lxe: write_utf8_parse_stringIO  (UATR T2)  173.5830 msec/pass
  cET: write_utf8_parse_stringIO  (UATR T2)  253.0849 msec/pass
  ET : write_utf8_parse_stringIO  (UATR T2)  519.2261 msec/pass

  lxe: write_utf8_parse_stringIO  (S-TR T3)    8.4269 msec/pass
  cET: write_utf8_parse_stringIO  (S-TR T3)   75.7639 msec/pass
  ET : write_utf8_parse_stringIO  (S-TR T3)  156.1930 msec/pass

  lxe: write_utf8_parse_stringIO  (SATR T4)    1.2100 msec/pass
  cET: write_utf8_parse_stringIO  (SATR T4)    6.4859 msec/pass
  ET : write_utf8_parse_stringIO  (SATR T4)    9.9051 msec/pass

For applications that require a high parser throughput and do little
serialization, cET is the best choice.  Also for iterparse
applications that extract small amounts of data from large XML data
sets.  If it comes to round-trip performance, however, lxml tends to
be between 30% and multiple times faster in total.  So, whenever the
input documents are not considerably bigger than the output, lxml is
the clear winner.


The ElementTree API
===================

Since all three libraries implement the same API, their performance is easy to
compare in this area.  A major disadvantage for lxml's performance is the
different tree model that underlies libxml2.  It allows lxml to provide parent
pointers for elements, but also increases the overhead of tree building and
restructuring.  This can be seen from the tree setup times of the benchmark
(given in seconds)::

  lxe:       --     S-     U-     -A     SA     UA  
       T1: 0.0783 0.0777 0.0774 0.0787 0.0781 0.0783
       T2: 0.0799 0.0796 0.0799 0.0879 0.0882 0.0886
       T3: 0.0245 0.0216 0.0217 0.0577 0.0575 0.0572
       T4: 0.0003 0.0003 0.0003 0.0011 0.0011 0.0011
  cET:       --     S-     U-     -A     SA     UA  
       T1: 0.0272 0.0264 0.0267 0.0268 0.0261 0.0265
       T2: 0.0280 0.0274 0.0273 0.0273 0.0276 0.0275
       T3: 0.0065 0.0066 0.0065 0.0111 0.0088 0.0088
       T4: 0.0001 0.0001 0.0001 0.0001 0.0001 0.0001
  ET :       --     S-     U-     -A     SA     UA  
       T1: 0.1302 0.1903 0.2208 0.1265 0.2542 0.1267
       T2: 0.2994 0.1301 0.3402 0.3746 0.1326 0.4170
       T3: 0.0301 0.0310 0.0302 0.0348 0.3654 0.0349
       T4: 0.0006 0.0005 0.0008 0.0006 0.0007 0.0006

While lxml is still faster than ET in most cases (10-70%), cET can be up to
three times faster than lxml here.  One of the reasons is that lxml must
additionally discard the created Python elements after their use, when they
are no longer referenced.  ET and cET represent the tree itself through these
objects, which reduces the overhead in creating them.


Child access
------------

The same reason makes operations like collecting children as in
``list(element)`` more costly in lxml.  Where ET and cET can quickly
create a shallow copy of their list of children, lxml has to create a
Python object for each child and collect them in a list::

  lxe: root_list_children        (--TR T1)    0.0160 msec/pass
  cET: root_list_children        (--TR T1)    0.0081 msec/pass
  ET : root_list_children        (--TR T1)    0.0541 msec/pass

  lxe: root_list_children        (--TR T2)    0.2100 msec/pass
  cET: root_list_children        (--TR T2)    0.0319 msec/pass
  ET : root_list_children        (--TR T2)    0.4420 msec/pass

This handicap is also visible when accessing single children::

  lxe: first_child               (--TR T2)    0.2429 msec/pass
  cET: first_child               (--TR T2)    0.2170 msec/pass
  ET : first_child               (--TR T2)    0.9968 msec/pass

  lxe: last_child                (--TR T1)    0.2470 msec/pass
  cET: last_child                (--TR T1)    0.2291 msec/pass
  ET : last_child                (--TR T1)    0.9830 msec/pass

... unless you also add the time to find a child index in a bigger
list.  ET and cET use Python lists here, which are based on arrays.
The data structure used by libxml2 is a linked tree, and thus, a
linked list of children::

  lxe: middle_child              (--TR T1)    0.2759 msec/pass
  cET: middle_child              (--TR T1)    0.2229 msec/pass
  ET : middle_child              (--TR T1)    1.0030 msec/pass

  lxe: middle_child              (--TR T2)    1.7071 msec/pass
  cET: middle_child              (--TR T2)    0.2229 msec/pass
  ET : middle_child              (--TR T2)    0.9930 msec/pass


Element creation
----------------

As opposed to ET, libxml2 has a notion of documents that each element must be
in.  This results in a major performance difference for creating independent
Elements that end up in independently created documents::

  lxe: create_elements           (--TC T2)    2.8961 msec/pass
  cET: create_elements           (--TC T2)    0.1929 msec/pass
  ET : create_elements           (--TC T2)    1.3590 msec/pass

Therefore, it is always preferable to create Elements for the document they
are supposed to end up in, either as SubElements of an Element or using the
explicit ``Element.makeelement()`` call::

  lxe: makeelement               (--TC T2)    1.9000 msec/pass
  cET: makeelement               (--TC T2)    0.3211 msec/pass
  ET : makeelement               (--TC T2)    1.6358 msec/pass

  lxe: create_subelements        (--TC T2)    1.7891 msec/pass
  cET: create_subelements        (--TC T2)    0.2351 msec/pass
  ET : create_subelements        (--TC T2)    3.2270 msec/pass

So, if the main performance bottleneck of an application is creating large XML
trees in memory through calls to Element and SubElement, cET is the best
choice.  Note, however, that the serialisation performance may even out this
advantage, especially for smaller trees and trees with many attributes.


Merging different sources
-------------------------

A critical action for lxml is moving elements between document contexts.  It
requires lxml to do recursive adaptations throughout the moved tree structure.

The following benchmark appends all root children of the second tree to the
root of the first tree::

  lxe: append_from_document      (--TR T1,T2)    3.3841 msec/pass
  cET: append_from_document      (--TR T1,T2)    0.2699 msec/pass
  ET : append_from_document      (--TR T1,T2)    1.2650 msec/pass

  lxe: append_from_document      (--TR T3,T4)    0.0441 msec/pass
  cET: append_from_document      (--TR T3,T4)    0.0169 msec/pass
  ET : append_from_document      (--TR T3,T4)    0.0820 msec/pass

Although these are fairly small numbers compared to parsing, this easily shows
the different performance classes for lxml and (c)ET.  Where the latter do not
have to care about parent pointers and tree structures, lxml has to deep
traverse the appended tree.  The performance difference therefore increases
with the size of the tree that is moved.

This difference is not always as visible, but applies to most parts of the
API, like inserting newly created elements::

  lxe: insert_from_document      (--TR T1,T2)    5.7020 msec/pass
  cET: insert_from_document      (--TR T1,T2)    0.4041 msec/pass
  ET : insert_from_document      (--TR T1,T2)    1.4789 msec/pass

or replacing the child slice by a newly created element::

  lxe: replace_children_element  (--TC T1)    0.2210 msec/pass
  cET: replace_children_element  (--TC T1)    0.0238 msec/pass
  ET : replace_children_element  (--TC T1)    0.1600 msec/pass

as opposed to replacing the slice with an existing element from the
same document::

  lxe: replace_children          (--TC T1)    0.0179 msec/pass
  cET: replace_children          (--TC T1)    0.0119 msec/pass
  ET : replace_children          (--TC T1)    0.0739 msec/pass

You should keep this difference in mind when you merge very large trees.


deepcopy
--------

Deep copying a tree is fast in lxml::

  lxe: deepcopy_all              (--TR T1)    9.7558 msec/pass
  cET: deepcopy_all              (--TR T1)  120.6188 msec/pass
  ET : deepcopy_all              (--TR T1)  902.6880 msec/pass

  lxe: deepcopy_all              (-ATR T2)   12.3210 msec/pass
  cET: deepcopy_all              (-ATR T2)  136.9810 msec/pass
  ET : deepcopy_all              (-ATR T2)  944.2801 msec/pass

  lxe: deepcopy_all              (S-TR T3)    8.3981 msec/pass
  cET: deepcopy_all              (S-TR T3)   35.6541 msec/pass
  ET : deepcopy_all              (S-TR T3)  221.6041 msec/pass

So, for example, if you have a database-like scenario where you parse in a
large tree and then search and copy independent subtrees from it for further
processing, lxml is by far the best choice here.


Tree traversal
--------------

Another area where lxml is very fast is iteration for tree traversal.  If your
algorithms can benefit from step-by-step traversal of the XML tree and
especially if few elements are of interest or the target element tag name is
known, lxml is a good choice::

  lxe: getiterator_all      (--TR T1)    5.7251 msec/pass
  cET: getiterator_all      (--TR T1)   39.9489 msec/pass
  ET : getiterator_all      (--TR T1)   23.0000 msec/pass

  lxe: getiterator_islice   (--TR T2)    0.0830 msec/pass
  cET: getiterator_islice   (--TR T2)    0.3440 msec/pass
  ET : getiterator_islice   (--TR T2)    0.2429 msec/pass

  lxe: getiterator_tag      (--TR T2)    0.3011 msec/pass
  cET: getiterator_tag      (--TR T2)   14.1001 msec/pass
  ET : getiterator_tag      (--TR T2)    7.4241 msec/pass

  lxe: getiterator_tag_all  (--TR T2)    0.6340 msec/pass
  cET: getiterator_tag_all  (--TR T2)   40.7901 msec/pass
  ET : getiterator_tag_all  (--TR T2)   21.0390 msec/pass

This translates directly into similar timings for ``Element.findall()``::

  lxe: findall              (--TR T2)    7.8950 msec/pass
  cET: findall              (--TR T2)   44.5340 msec/pass
  ET : findall              (--TR T2)   27.1149 msec/pass

  lxe: findall              (--TR T3)    1.7281 msec/pass
  cET: findall              (--TR T3)   12.9611 msec/pass
  ET : findall              (--TR T3)    8.6131 msec/pass

  lxe: findall_tag          (--TR T2)    0.7720 msec/pass
  cET: findall_tag          (--TR T2)   40.6358 msec/pass
  ET : findall_tag          (--TR T2)   21.4581 msec/pass

  lxe: findall_tag          (--TR T3)    0.2050 msec/pass
  cET: findall_tag          (--TR T3)    9.6831 msec/pass
  ET : findall_tag          (--TR T3)    5.2109 msec/pass

Note that all three libraries currently use the same Python implementation for
``findall()``, except for their native tree iterator.


XPath
=====

The following timings are based on the benchmark script `bench_xpath.py`_.

This part of lxml does not have an equivalent in ElementTree.  However, lxml
provides more than one way of accessing it and you should take care which part
of the lxml API you use.  The most straight forward way is to call the
``xpath()`` method on an Element or ElementTree::

  lxe: xpath_method         (--TC T1)    1.7459 msec/pass
  lxe: xpath_method         (--TC T2)   22.0850 msec/pass
  lxe: xpath_method         (--TC T3)    0.1309 msec/pass
  lxe: xpath_method         (--TC T4)    1.0772 msec/pass

This is well suited for testing and when the XPath expressions are as diverse
as the trees they are called on.  However, if you have a single XPath
expression that you want to apply to a larger number of different elements,
the ``XPath`` class is the most efficient way to do it::

  lxe: xpath_class          (--TC T1)    0.6740 msec/pass
  lxe: xpath_class          (--TC T2)    3.1760 msec/pass
  lxe: xpath_class          (--TC T3)    0.0548 msec/pass
  lxe: xpath_class          (--TC T4)    0.1700 msec/pass

Note that this still allows you to use variables in the expression, so you can
parse it once and then adapt it through variables at call time.  In other
cases, where you have a fixed Element or ElementTree and want to run different
expressions on it, you should consider the ``XPathEvaluator``::

  lxe: xpath_element        (--TR T1)    0.4151 msec/pass
  lxe: xpath_element        (--TR T2)   11.6129 msec/pass
  lxe: xpath_element        (--TR T3)    0.1299 msec/pass
  lxe: xpath_element        (--TR T4)    0.3409 msec/pass

While it looks slightly slower, creating an XPath object for each of the
expressions generates a much higher overhead here::

  lxe: xpath_class_repeat   (--TC T1)    1.6699 msec/pass
  lxe: xpath_class_repeat   (--TC T2)   20.4420 msec/pass
  lxe: xpath_class_repeat   (--TC T3)    0.1230 msec/pass
  lxe: xpath_class_repeat   (--TC T4)    0.9859 msec/pass


A longer example
================

... based on lxml 1.3.

A while ago, Uche Ogbuji posted a `benchmark proposal`_ that would
read in a 3MB XML version of the `Old Testament`_ of the Bible and
look for the word *begat* in all verses.  Apparently, it is contained
in 120 out of almost 24000 verses.  This is easy to implement in
ElementTree using ``findall()``.  However, the fastest and most memory
friendly way to do this is obviously ``iterparse()``, as most of the
data is not of any interest.

.. _`benchmark proposal`: http://www.onlamp.com/pub/wlg/6291
.. _`Old Testament`: http://www.ibiblio.org/bosak/xml/eg/religion.2.00.xml.zip

Now, Uche's original proposal was more or less the following::

  def bench_ET():
      tree = ElementTree.parse("ot.xml")
      result = []
      for v in tree.findall("//v"):
          text = v.text
          if 'begat' in text:
              result.append(text)
      return len(result)

which takes about one second on my machine today.  The faster ``iterparse()``
variant looks like this::

  def bench_ET_iterparse():
      result = []
      for event, v in ElementTree.iterparse("ot.xml"):
          if v.tag == 'v':
              text = v.text
              if 'begat' in text:
                  result.append(text)
          v.clear()
      return len(result)

The improvement is about 10%.  At the time I first tried (early 2006), lxml
didn't have ``iterparse()`` support, but the ``findall()`` variant was already
faster than ElementTree.  This changes immediately when you switch to
cElementTree.  The latter only needs 0.17 seconds to do the trick today and
only some impressive 0.10 seconds when running the iterparse version.  And
even back then, it was quite a bit faster than what lxml could achieve.

Since then, lxml has matured a lot and has gotten much faster.  The iterparse
variant now runs in 0.14 seconds, and if you remove the ``v.clear()``, it is
even a little faster (which isn't the case for cElementTree).

One of the many great tools in lxml is XPath, a swiss army knife for finding
things in XML documents.  It is possible to move the whole thing to a pure
XPath implementation, which looks like this::

  def bench_lxml_xpath_all():
      tree = etree.parse("ot.xml")
      result = tree.xpath("//v[contains(., 'begat')]/text()")
      return len(result)

This runs in about 0.13 seconds and is about the shortest possible
implementation (in lines of Python code) that I could come up with.  Now, this
is already a rather complex XPath expression compared to the simple "//v"
ElementPath expression we started with.  Since this is also valid XPath, let's
try this instead::

  def bench_lxml_xpath():
      tree = etree.parse("ot.xml")
      result = []
      for v in tree.xpath("//v"):
          text = v.text
          if 'begat' in text:
              result.append(text)
      return len(result)

This gets us down to 0.12 seconds, thus showing that a generic XPath
evaluation engine cannot always compete with a simpler, tailored solution.
However, since this is not much different from the original findall variant,
we can remove the complexity of the XPath call completely and just go with
what we had in the beginning.  Under lxml, this runs in the same 0.12 seconds.

But there is one thing left to try.  We can replace the simple ElementPath
expression with a native tree iterator::

  def bench_lxml_getiterator():
      tree = etree.parse("ot.xml")
      result = []
      for v in tree.getiterator("v"):
          text = v.text
          if 'begat' in text:
              result.append(text)
      return len(result)

This implements the same thing, just without the overhead of parsing and
evaluating a path expression.  And this makes it another bit faster, down to
0.11 seconds.  For comparison, cElementTree runs this version in 0.17 seconds.

So, what have we learned?

* Python code is not slow.  The pure XPath solution was not even as fast as
  the first shot Python implementation.  In general, a few more lines in
  Python make things more readable, which is much more important than the last
  5% of performance.

* It's important to know the available options - and it's worth starting with
  the most simple one.  In this case, a programmer would then probably have
  started with ``getiterator("v")`` or ``iterparse()``.  Either of them would
  already have been the most efficient, depending on which library is used.

* It's important to know your tool.  lxml and cElementTree are both very fast
  libraries, but they do not have the same performance characteristics.  The
  fastest solution in one library can be comparatively slow in the other.  If
  you optimise, optimise for the specific target platform.

* It's not always worth optimising.  After all that hassle we got from 0.12
  seconds for the initial implementation to 0.11 seconds.  Switching over to
  cElementTree and writing an ``iterparse()`` based version would have given
  us 0.10 seconds - not a big difference for 3MB of XML.

* Take care what operation is really dominating in your use case.  If we split
  up the operations, we can see that lxml is slightly slower than cElementTree
  on ``parse()`` (both about 0.06 seconds), but more visibly slower on
  ``iterparse()``: 0.07 versus 0.10 seconds.  However, tree iteration in lxml
  is increadibly fast, so it can be better to parse the whole tree and then
  iterate over it rather than using ``iterparse()`` to do both in one step.
  Or, you can just wait for the lxml authors to optimise iterparse in one of
  the next releases...


lxml.objectify
==============

The following timings are based on the benchmark script `bench_objectify.py`_.

Objectify is a data-binding API for XML based on lxml.etree, that was added in
version 1.1.  It uses standard Python attribute access to traverse the XML
tree.  It also features ObjectPath, a fast path language based on the same
meme.

Just like lxml.etree, lxml.objectify creates Python representations of
elements on the fly.  To save memory, the normal Python garbage collection
mechanisms will discard them when their last reference is gone.  In cases
where deeply nested elements are frequently accessed through the objectify
API, the create-discard cycles can become a bottleneck, as elements have to be
instantiated over and over again.


ObjectPath
----------

ObjectPath can be used to speed up the access to elements that are deep in the
tree.  It avoids step-by-step Python element instantiations along the path,
which can substantially improve the access time::

  lxe: attribute                  (--TR T1)    9.4581 msec/pass
  lxe: attribute                  (--TR T2)   52.5560 msec/pass
  lxe: attribute                  (--TR T4)    9.1729 msec/pass

  lxe: objectpath                 (--TR T1)    4.8690 msec/pass
  lxe: objectpath                 (--TR T2)   47.8780 msec/pass
  lxe: objectpath                 (--TR T4)    4.7870 msec/pass

  lxe: attributes_deep            (--TR T1)   54.7471 msec/pass
  lxe: attributes_deep            (--TR T2)   62.7451 msec/pass
  lxe: attributes_deep            (--TR T4)   15.1050 msec/pass

  lxe: objectpath_deep            (--TR T1)   48.2810 msec/pass
  lxe: objectpath_deep            (--TR T2)   51.3949 msec/pass
  lxe: objectpath_deep            (--TR T4)    6.1419 msec/pass

Note, however, that parsing ObjectPath expressions is not for free either, so
this is most effective for frequently accessing the same element.


Caching Elements
----------------

A way to improve the normal attribute access time is static instantiation of
the Python objects, thus trading memory for speed.  Just create a cache
dictionary and run::

    cache[root] = list(root.iter())

after parsing and::

    del cache[root]

when you are done with the tree.  This will keep the Python element
representations of all elements alive and thus avoid the overhead of repeated
Python object creation.  You can also consider using filters or generator
expressions to be more selective.  By choosing the right trees (or even
subtrees and elements) to cache, you can trade memory usage against access
speed::

  lxe: attribute_cached           (--TR T1)    7.5061 msec/pass
  lxe: attribute_cached           (--TR T2)   50.1881 msec/pass
  lxe: attribute_cached           (--TR T4)    7.4170 msec/pass

  lxe: attributes_deep_cached     (--TR T1)   48.7239 msec/pass
  lxe: attributes_deep_cached     (--TR T2)   55.2199 msec/pass
  lxe: attributes_deep_cached     (--TR T4)    9.9740 msec/pass

  lxe: objectpath_deep_cached     (--TR T1)   43.4160 msec/pass
  lxe: objectpath_deep_cached     (--TR T2)   47.6480 msec/pass
  lxe: objectpath_deep_cached     (--TR T4)    3.4680 msec/pass

Things to note: you cannot currently use ``weakref.WeakKeyDictionary`` objects
for this as lxml's element objects do not support weak references (which are
costly in terms of memory).  Also note that new element objects that you add
to these trees will not turn up in the cache automatically and will therefore
still be garbage collected when all their Python references are gone, so this
is most effective for largely immutable trees.  You should consider using a
set instead of a list in this case and add new elements by hand.


Further optimisations
---------------------

Here are some more things to try if optimisation is required:

* A lot of time is usually spent in tree traversal to find the addressed
  elements in the tree.  If you often work in subtrees, do what you would also
  do with deep Python objects: assign the parent of the subtree to a variable
  or pass it into functions instead of starting at the root.  This allows
  accessing its descendents more directly.

* Try assigning data values directly to attributes instead of passing them
  through DataElement.

* If you use custom data types that are costly to parse, try running
  ``objectify.annotate()`` over read-only trees to speed up the attribute type
  inference on read access.

Note that none of these measures is guaranteed to speed up your application.
As usual, you should prefer readable code over premature optimisations and
profile your expected use cases before bothering to apply optimisations at
random.
