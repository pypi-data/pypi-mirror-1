\File{(stdin)}{Fri Sep 26 11:45:33 2008}

\PythonNoMath

\PythonDocBlock
\verb""\\ 
\verb"A module implementing various techniques for deconvolving an image by a"\\ 
\verb"kernel.  Currently implemented are Clean, Least-Squares, Maximum Entropy,"\\ 
\verb"and Annealing.  Standard parameters to these functions are:"\\ 
\verb"im = image to be deconvolved."\\ 
\verb"ker = kernel to deconvolve by (must be same size as im)."\\ 
\verb"mdl = a priori model of what the deconvolved image should look like."\\ 
\verb"maxiter = maximum number of iterations performed before terminating."\\ 
\verb"tol = termination criterion, lower being more optimized."\\ 
\verb"verbose =  print info on how things are progressing."\\ 
\verb"lower = lower bound of pixel values in deconvolved image"\\ 
\verb"upper = upper bound of pixel values in deconvolved image"\\ 

\PythonDocBlockEnd


\I{20}{0}$\K{def}\ \F{clean1d}\,(\V{im},\V{ker},\V{mdl}\Y{=}\V{None},\V{gain}\Y{=}.\NUM{1}{},\V{maxiter}\Y{=}\NUM{10000}{},\V{tol}\Y{=}\EXP{1}{-3}{})\Y{:}$
\DS{21}{1}{%
\verb"Perform 1 dimensional deconvolution using the CLEAN algorithm."}\vskip 5pt 














\I{37}{0}$\K{def}\ \F{recenter}\,(a,c)\Y{:}$
\DS{38}{1}{%
\verb"Slide the (0,0) point of matrix a to a new location tuple c."}\vskip 5pt 





\I{45}{0}$\K{def}\ \F{clean}\,(\V{im},\V{ker},\V{mdl}\Y{=}\V{None},\V{gain}\Y{=}.\NUM{2}{},\V{maxiter}\Y{=}\NUM{10000}{},\V{chkiter}\Y{=}\NUM{100}{},\V{tol}\Y{=}\EXP{1}{-3}{},$

\DS{47}{1}{%
\verb"This standard Hoegbom clean deconvolution algorithm operates on the "\\ 
\verb"assumption that the image is composed of point sources.  This makes it a "\\ 
\verb"poor choice for images with distributed flux.  In each iteration, a point "\\ 
\verb"is added to the model at the location of the maximum residual, with a "\\ 
\verb"fraction (specified by 'gain') of the magnitude.  The convolution of that "\\ 
\verb"point is removed from the residual, and the process repeats.  Termination "\\ 
\verb"happens after 'maxiter' iterations, or when the clean loops starts "\\ 
\verb"increasing the magnitude of the residual."\\ 
\verb"gain: The fraction of a residual used in each iteration.  If this is too"\\ 
\verb"    low, clean takes unnecessarily long.  If it is too high, clean does"\\ 
\verb"    a poor job of deconvolving."\\ 
\verb"chkiter: The number of iterations between when clean checks if the "\\ 
\verb"    residual is increasing."}\vskip 5pt 





























\I{100}{0}$\K{def}\ \F{lsq}\,(\V{im},\V{ker},\V{mdl}\Y{=}\V{None},\V{gain}\Y{=}.\NUM{1}{},\V{tol}\Y{=}\EXP{1}{-3}{},\V{maxiter}\Y{=}\NUM{200}{},$

\DS{102}{1}{%
\verb"This simple least-square fitting procedure for deconvolving an image "\\ 
\verb"saves computing by assuming a diagonal pixel-pixel gradient of the fit."\\ 
\verb"In essence, this assumes that the convolution kernel is a delta-function."\\ 
\verb"This works for small kernels, but not so well for large ones.  See Cornwell "\\ 
\verb"and Evans, 1984 ""\verb"A Simple Maximum Entropy Deconvolution Algorithm""\verb" for more "\\ 
\verb"information about this approximation.  Unlike maximum entropy, lsq makes "\\ 
\verb"no promises about maximizing smoothness, but needs no information"\\ 
\verb"about noise levels.  Structure can be introduced for which there is no "\\ 
\verb"evidence in the original image.  Termination happens when the fractional "\\ 
\verb"score change is less than 'tol' between iterations."\\ 
\verb"gain: The fraction of the step size (calculated from the gradient) taken"\\ 
\verb"    in each iteration.  If this is too low, the fit takes unnecessarily "\\ 
\verb"    long.  If it is too high, the fit process can oscillate."}\vskip 5pt 





\I{122}{1}$\K{def}\ \F{f}\,(x)\Y{:}$























\I{150}{0}$\K{def}\ \F{maxent}\,(\V{im},\V{ker},\V{var0},\V{mdl}\Y{=}\V{None},\V{gain}\Y{=}.\NUM{1}{},\V{tol}\Y{=}\EXP{1}{-3}{},\V{maxiter}\Y{=}\NUM{200}{},$

\DS{152}{1}{%
\verb"Maximum entropy deconvolution (MEM) (see Cornwell and Evans 1984"\\ 
"\verb"A Simple Maximum Entropy Deconvolution Algorithm""\verb" and Sault 1990"\\ 
"\verb"A Modification of the Cornwell and Evans Maximum Entropy Algorithm""\verb")"\\ 
\verb"is similar to lsq, but the fit is only optimized to within the specified"\\ 
\verb"variance (var0) and then ""\verb"smoothness""\verb" is maximized.  This has several "\\ 
\verb"desirable effects including uniqueness of solution, equal weighting of"\\ 
\verb"Fourier components, and absence of spurious structure.  The same "\\ 
\verb"delta-kernel approximation (see lsq) is made here."\\ 
\verb"var0: The estimated variance (noise power) in the image.  If none is"\\ 
\verb"    provided, a quick lsq is used to estimate the variance of the residual."\\ 
\verb"gain: The fraction of the step size (calculated from the gradient) taken"\\ 
\verb"    in each iteration.  If this is too low, the fit takes unnecessarily "\\ 
\verb"    long.  If it is too high, the fit process can oscillate."}\vskip 5pt 








\I{174}{1}$\K{def}\ \F{next\_step}\,(\V{b\_i},\V{alpha},\V{verbose}\Y{=}\V{False})\Y{:}$








\I{184}{2}$\K{def}\ \F{dot}\,(x,y)\Y{:}\ \K{return}\ (x\Y{*}y\Y{/}\Y{-}\V{gg\_J}).\F{sum}\,()$

























\I{213}{0}$\K{def}\ \F{maxent\_findvar}\,(\V{im},\V{ker},\V{var}\Y{=}\V{None},\V{f\_var0}\Y{=}.\NUM{6}{},\V{mdl}\Y{=}\V{None},\V{gain}\Y{=}.\NUM{1}{},\V{tol}\Y{=}\EXP{1}{-3}{},$


\DS{216}{1}{%
\verb"This frontend to maxent tries to find a variance for which maxent will"\\ 
\verb"converge.  If the starting variance (var) is not specified, it will be"\\ 
\verb"estimated as a fraction (f_var0) of the variance of the residual of a "\\ 
\verb"lsq deconvolution, and then a search algorithm tests an ever-widening"\\ 
\verb"range around that value.  This function will search until it succeeds."}\vskip 5pt 






























\I{254}{0}$\K{def}\ \F{anneal}\,(\V{im},\V{ker},\V{mdl}\Y{=}\V{None},\V{maxiter}\Y{=}\NUM{1000}{},\V{lower}\Y{=}\V{lo\_clip\_lev},\V{upper}\Y{=}n.\V{Inf},$

\DS{256}{1}{%
\verb"Annealing takes a non-deterministic approach to deconvolution by"\\ 
\verb"randomly perturbing the model and selecting perturbations that improve the "\\ 
\verb"residual.  By slowly reducing the temperature of the perturbations, "\\ 
\verb"annealing attempts to settle into a global minimum.  Annealing is slower"\\ 
\verb"than lsq for a known gradient, but is less sensitive to gradient errors "\\ 
\verb"(it can solve for wider kernels). Faster cooling speeds terminate more "\\ 
\verb"quickly, but are less likely to find the global minimum.  This "\\ 
\verb"implementation assigns a temperature to each pixel proportional to the "\\ 
\verb"magnitude of the residual in that pixel and the global cooling speed."\\ 
\verb"cooling: A function accepting (iteration,residuals) that returns a "\\ 
\verb"    vector of standard deviation for noise in the respective pixels."\\ 
\verb"    Picking the scaling of this function correctly is vital for annealing"\\ 
\verb"    to work."}\vskip 5pt 















